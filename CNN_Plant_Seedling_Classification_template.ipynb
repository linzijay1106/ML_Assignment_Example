{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13-q8sl6nrgPSB8w0apJ9WK89V2CPNiiy","timestamp":1710729568554},{"file_id":"13QVr6Vy7eCHhS4yzsI3mo7AEYzcrEsmT","timestamp":1692393708795}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HCOshj1zQMIV"},"source":["# CNN_Plant_Seedling_Classification\n","\n","\n","此範例會使用模組化的軟體設計方法，它將一個大型的程式或專案拆分成更小、更容易管理的模塊或組件。每個模塊執行特定的功能或處理特定的任務，並且可以獨立開發、測試和維護。這樣的設計有助於提高程式碼的可讀性、可重用性和可維護性，並且讓不同團隊的開發者能夠協作更輕鬆。在PyTorch中，模組化通常表現為創建獨立的模型、函數和類別，每個模塊負責特定的任務，並且可以輕鬆組合在一起以構建更大的深度學習模型。\n","\n","本次課程將會把pytorch訓練流程分成以下幾個模組來教學:\n","1. `Dataset`\n","2. `Dataloader`\n","3. `Model`\n","4. `Train` Function\n","5. `Valid` Function\n","6. `Plot Curve` Function\n","7. `Predict` Function\n","8. `Main` Function\n","9. `Addition` Customize model"]},{"cell_type":"markdown","metadata":{"id":"XOozzhScRUW0"},"source":["## Download Datasets\n","\n","此範例使用kaggle上的[Plant Seedlings Classification](https://www.kaggle.com/c/plant-seedlings-classification)資料集，請先至網站中下載`plant-seedlings-classification.zip`，透過`google.colab`套件，我們可以讓 Colab 上的程式直接讀取自己的雲端硬碟。\n","\n","\n","執行下面的code之前，請先確保自己的google drive中已經有`plant-seedlings-classification.zip`檔案，並確保檔案位置正確。\n","\n","掛載自己的google drive後，雲端硬碟的根目錄為: `/content/gdrive/MyDrive/`\n","\n"]},{"cell_type":"code","source":["import os\n","import zipfile\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","data_dir = '/content/gdrive/MyDrive/Colab_Notebooks/plant_seedlings_classification/' # dataset's dir you want to unzip\n","\n","if not os.path.exists(data_dir):\n","  zip_dir = '/content/gdrive/MyDrive/Colab_Notebooks/plant-seedlings-classification.zip' # your zip file's dir\n","\n","  with zipfile.ZipFile(zip_dir, 'r') as zip_ref:\n","      zip_ref.extractall(data_dir)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"1O82OWk7PFCE","executionInfo":{"status":"error","timestamp":1710730152436,"user_tz":-480,"elapsed":40654,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}},"outputId":"7c05834b-04f2-41f3-9b4a-512d31161e76"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/gdrive/MyDrive/Colab_Notebooks/plant-seedlings-classification.zip'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-90d2c43d7a0b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mzip_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/MyDrive/Colab_Notebooks/plant-seedlings-classification.zip'\u001b[0m \u001b[0;31m# your zip file's dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/Colab_Notebooks/plant-seedlings-classification.zip'"]}]},{"cell_type":"markdown","source":["## Import Package"],"metadata":{"id":"LSIrU_dAIIID"}},{"cell_type":"code","metadata":{"id":"t4wB0uKSBaR2","executionInfo":{"status":"aborted","timestamp":1710730152437,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torchvision import models\n","from torchvision.utils import make_grid\n","from torchvision import transforms as tsfm\n","from torchvision.datasets import ImageFolder\n","\n","import random\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from PIL import Image\n","from pathlib import Path\n","from IPython import display\n","\n","# Set random seed for reproducibility\n","manualSeed = 999\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Config\n","Config of Hyperparameter"],"metadata":{"id":"0a-lDD09IPFP"}},{"cell_type":"code","source":["# Set Hyperparameters\n","batch_size = 64\n","epochs = 50\n","learning_rate = 0.001\n","train_dir = os.path.join(data_dir, 'train')\n","test_dir = os.path.join(data_dir, 'test')"],"metadata":{"id":"13gLt3SPISiT","executionInfo":{"status":"aborted","timestamp":1710730152437,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"prY6qLtPRj55"},"source":["## 1. Custom Pytorch Dataset\n","\n","\n","A `Dataset` in Pytorch should have three methods:\n","\n","1. `__init__`: Read data & preprocess\n","2. `__len__`: return a integer indicating the size of the dataset\n","3. `__getitem__`: given an index `idx`, return the `idx`-th sample\n","\n","In our case, the download data is structured as:\n","```\n","├── train\n","│   ├── Black-grass (Class name)\n","│   │   ├── 0050f38b3.png\n","│   │   ├── 0183fdf68.png\n","│   │   ├── 0260cffa8.png\n","│   │   ├── ...\n","│   └── Charlock\n","│   │   ├── 022179d65.png\n","│   │   ├── 02c95e601.png\n","│   │   ├── 04098447d.png\n","│   │   ├── ...\n","│   └── ...\n","├── test\n","│   ├── 0021e90e4.png\n","│   ├── 003d61042.png\n","│   ├── 007b3da8b.png\n","│   ├── ...\n","```\n","\n","We can simply find all the pngs and load them into memory when needed. In common practice, dataset will receive one or more `torchvision.transforms` which transform the png (loaded as `PIL.Image`) into pytorch tensor.\n","\n","`Dataloader` batchify the samples in dataset, i.e. builds mini-batch from the data return by dataset's `__getitem__` . We then iterate the `Dataloader` for training.\n","\n","Usually `Dataloader` is finite and will run of mini-batches when we have seen all samples in `Dataset` once. However, to write our code easily, we can create an infinite `Sampler` that can guide the batchification process in `Dataloader` and make `Dataloader` supply mini-batches infinitely."]},{"cell_type":"code","metadata":{"id":"c6wV3S8ovtlE","executionInfo":{"status":"aborted","timestamp":1710730152437,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"source":["class Train_data(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.data = ImageFolder(root=root_dir, transform=transform)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img, label = self.data[idx]\n","        return img, label\n","\n","class Pred_data(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.img_paths = list(Path(root_dir).glob('*.png'))\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.img_paths[idx])\n","        img = self.transform(img)\n","        img = img.unsqueeze(0)\n","        return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize dataset item for debug\n","train_dir = os.path.join(data_dir, 'train')\n","test_dir = os.path.join(data_dir, 'test')\n","transform = tsfm.Compose([\n","    tsfm.Resize((224, 224)),\n","    tsfm.ToTensor(),\n","])\n","\n","whole_set = Train_data(\n","    root_dir=train_dir,\n","    transform=transform\n",")\n","\n","test_set = Pred_data(\n","    root_dir=test_dir,\n","    transform=transform\n",")\n","\n","num_images_to_display = 5\n","fig, axs = plt.subplots(1, num_images_to_display, figsize=(15, 3))\n","\n","for i, (img, label) in enumerate(whole_set):\n","    axs[i].imshow(img.permute(1, 2, 0))\n","    axs[i].set_title(f'Class: {label}')\n","    axs[i].axis('off')\n","\n","    num_images_to_display -= 1\n","    if num_images_to_display == 0:\n","        break\n","\n","plt.tight_layout()\n","plt.show()\n","\n","num_images_to_display = 5\n","fig, axs = plt.subplots(1, num_images_to_display, figsize=(15, 3))\n","for i, img in enumerate(test_set):\n","    axs[i].imshow(img[0].permute(1, 2, 0))\n","    axs[i].set_title(f'Test img: {i}')\n","    axs[i].axis('off')\n","\n","    num_images_to_display -= 1\n","    if num_images_to_display == 0:\n","        break\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"tOGcORfOT2Yz","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Split train, valid set and Create Dataloader:\n"],"metadata":{"id":"weRBnmJ2UxF-"}},{"cell_type":"code","metadata":{"id":"cDelZWMpDB9i","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"source":["train_set, valid_set = random_split(whole_set, [0.8, 0.2])\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_set, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrdV2X93Yh9E"},"source":["## 3. Create Model\n","\n","A `nn.Module` in Pytorch should have two methods:\n","\n","1. `__init__`: Initialize your model & define layers\n","2. `forward`: Compute output of your Model\n","\n"]},{"cell_type":"code","metadata":{"id":"dBv5LjNGDq8L","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"source":["class resnet_50(nn.Module):\n","    def __init__(self, num_classes = 12):\n","        super(resnet_50, self).__init__()\n","        # pytorch built-in models\n","        self.resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n","\n","        # set model layers trainable\n","        for param in self.resnet50.parameters():\n","            param.requires_grad = True\n","\n","        # redifine/customize last classification layer\n","        self.resnet50.fc = nn.Linear(2048, num_classes)\n","\n","    def forward(self, x):\n","        x = self.resnet50(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test model for debug\n","\n","model = resnet_50(num_classes=12).cuda()\n","# print(model)\n","x = torch.rand(1, 3, 224, 224).cuda()\n","y = model(x)\n","print(x)\n","print(y)"],"metadata":{"id":"cI969Z-K1Gn7","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UkxgGKVwdyda"},"source":["## 4. Define Train Function(for one epoch):\n"]},{"cell_type":"code","source":["def train(model, criterion, optimizer, train_loader, epoch, total_epochs, batch_size):\n","    model.train()\n","    train_loss, train_acc = [], []\n","\n","    tqdm_iter = tqdm(train_loader, desc=\"Epoch: {}/{} ({}%) | Training loss: NaN\".format(\n","    epoch, total_epochs, int(epoch/total_epochs * 100)), leave=False)\n","    epoch_loss, epoch_acc = 0.0, 0.0\n","    for batch_idx, (data, label) in enumerate(tqdm_iter):\n","        data, target = data.cuda(), label.cuda()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        acc = (output.argmax(dim=1) == target).float().mean().item()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc\n","\n","        tqdm_iter.set_description(\"Epoch: {}/{} ({}%) | Training loss: {:.6f} | Training Acc: {:.6f}\".format(\n","        epoch + 1, total_epochs, int((epoch+1)/total_epochs * 100), round(loss.item(), 6), round(acc, 6)))\n","\n","    return epoch_loss / len(train_loader), epoch_acc / len(train_loader)"],"metadata":{"id":"wmY5HWvtqIXn","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# debug \"train\" function\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","loss, acc = train(\n","    model,\n","    criterion,\n","    optimizer,\n","    train_loader,\n","    epoch=1,\n","    total_epochs=1,\n","    batch_size=batch_size\n",")\n","\n","print(loss, acc)"],"metadata":{"id":"13uX3REdWr4K","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Define Valid Function(for one epoch):\n"],"metadata":{"id":"RiL9Kb0PKvwl"}},{"cell_type":"code","metadata":{"id":"V29iauDlu4t5","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"source":["def valid(model, criterion, valid_loader, epoch, total_epochs, batch_size):\n","    model.eval()\n","\n","    tqdm_iter = tqdm(valid_loader, desc=\"Epoch: {}/{} ({}%) | Valid loss: NaN\".format(\n","    epoch, total_epochs, int(epoch/total_epochs * 100)), leave=False)\n","    epoch_loss, epoch_acc = 0.0, 0.0\n","    with torch.no_grad():\n","        for batch_idx, (data, label) in enumerate(tqdm_iter):\n","            data, target = data.cuda(), label.cuda()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            acc = (output.argmax(dim=1) == target).float().mean().item()\n","            epoch_loss += loss.item()\n","            epoch_acc += acc\n","\n","            tqdm_iter.set_description(\"Epoch: {}/{} ({}%) | Valid loss: {:.6f} | Valid Acc: {:.6f}\".format(\n","            epoch + 1, total_epochs, int((epoch+1)/total_epochs * 100), round(loss.item(), 6), round(acc, 6)))\n","\n","    return epoch_loss / len(valid_loader), epoch_acc / len(valid_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# debug \"valid\" function\n","criterion = nn.CrossEntropyLoss()\n","loss, acc = valid(\n","    model,\n","    criterion,\n","    valid_loader,\n","    epoch=1,\n","    total_epochs=1,\n","    batch_size=batch_size\n",")\n","\n","print(loss, acc)"],"metadata":{"id":"aFmHlyWtW5tj","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":5,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Plot Learning Curve Function:"],"metadata":{"id":"lFRQ5iuQQydh"}},{"cell_type":"code","metadata":{"id":"VXNZ6owLloYe","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":5,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"source":["def Plot(title, ylabel, epochs, train_loss, valid_loss):\n","    plt.figure()\n","    plt.title(title)\n","    plt.xlabel('epochs')\n","    plt.ylabel(ylabel)\n","    plt.plot(epochs, train_loss)\n","    plt.plot(epochs, valid_loss)\n","    plt.legend(['train', 'valid'], loc='upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# debug \"Plot\" function\n","debug_epochs = [1, 2, 3, 4, 5]\n","debug_train_loss = [0.1, 0.08, 0.06, 0.05, 0.04]\n","debug_valid_loss = [0.2, 0.15, 0.12, 0.1, 0.09]\n","\n","Plot(\"Training and Validation Loss\", 'Loss', debug_epochs, debug_train_loss, debug_valid_loss)\n","\n","plt.show()"],"metadata":{"id":"bOjwmmscaHtL","executionInfo":{"status":"aborted","timestamp":1710730152438,"user_tz":-480,"elapsed":5,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Predict Function:"],"metadata":{"id":"zFPmxmdOZLu6"}},{"cell_type":"code","source":["def predict(loader, model):\n","    model.eval()\n","    preds = []\n","    for data in tqdm(loader):\n","        pred = model(data.cuda())\n","        cls = torch.argmax(pred, dim=1)\n","        preds.append(cls)\n","\n","    return preds"],"metadata":{"id":"qNpjt-afZLOV","executionInfo":{"status":"aborted","timestamp":1710730152439,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize Predict result\n","def view_pred_result(preds, num_images_to_display=5):\n","    labels = ['Black-grass', 'Charlock' , 'Cleavers' , 'Common Chickweed' , 'Common wheat' , 'Fat Hen' , 'Loose Silky-bent' , 'Maize' , 'Scentless Mayweed' , 'Shepherds Purse', 'Small-flowered Cranesbill' , 'Sugar beet']\n","    fig, axs = plt.subplots(1, num_images_to_display, figsize=(15, 3))\n","    for i, img in enumerate(test_set):\n","        axs[i].imshow(img[0].permute(1, 2, 0))\n","        axs[i].set_title(labels[preds[i].item()])\n","        axs[i].axis('off')\n","\n","        num_images_to_display -= 1\n","        if num_images_to_display == 0:\n","            break\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"OIZ5WD5pvBRd","executionInfo":{"status":"aborted","timestamp":1710730152439,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# debug \"Predict\" function & \"View_Predict_result\" function\n","test_dir = os.path.join(data_dir, 'test')\n","transform = tsfm.Compose([\n","    tsfm.Resize((224, 224)),\n","    tsfm.ToTensor(),\n","])\n","test_set = Pred_data(\n","    root_dir=test_dir,\n","    transform=transform\n",")\n","model = resnet_50(num_classes=12).cuda()\n","\n","preds = predict(test_set, model)\n","view_pred_result(preds)"],"metadata":{"id":"_c-diUny6Vz0","executionInfo":{"status":"aborted","timestamp":1710730152439,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 8. Main Function(training pipeline):\n","1. `Set Hyperparameters`: `batct_size`, `learning rate`, `epochs`...\n","2. `Initial`: initial `dataset`, `dataloader`, `model`\n","3. `Train`: Do train\n","4. `Valid`: Do valid\n","5. repeat `3.`  `4.` epochs times\n","6. `Plot curve`: Plot learning curve to observe the learning progress\n","7. `Predict`: Use the trained model to predict the results of the test set"],"metadata":{"id":"rDIbCKKe89C-"}},{"cell_type":"code","source":["def main():\n","    # initial transform\n","    transform = tsfm.Compose([\n","        tsfm.Resize((224, 224)),\n","        tsfm.ToTensor(),\n","    ])\n","\n","    # initial dataset\n","    whole_set = Train_data(\n","        root_dir=train_dir,\n","        transform=transform\n","    )\n","\n","    test_set = Pred_data(\n","        root_dir=test_dir,\n","        transform=transform\n","    )\n","\n","    # split train valid and initial dataloader\n","    train_set_size = int(len(whole_set) * 0.8)\n","    valid_set_size = len(whole_set) - train_set_size\n","    train_set, valid_set = random_split(whole_set, [train_set_size, valid_set_size])\n","\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    valid_loader = DataLoader(valid_set, batch_size=batch_size)\n","\n","    # initial model\n","    model = resnet_50(num_classes=12).cuda()\n","\n","    # initial loss_function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # initial plot values\n","    train_loss, train_acc = [], []\n","    valid_loss, valid_acc = [], []\n","    epoch_list = []\n","\n","    # repeat train and valid epochs times\n","    print(epochs)\n","    for epoch in range(epochs):\n","      epoch_list.append(epoch + 1)\n","\n","      loss, acc = train(\n","          model,\n","          criterion,\n","          optimizer,\n","          train_loader,\n","          epoch=epoch,\n","          total_epochs=epochs,\n","          batch_size=batch_size\n","      )\n","      train_loss.append(loss)\n","      train_acc.append(acc)\n","      print(f'Avg train Loss: {loss}, Avg train acc: {acc}')\n","\n","      loss, acc = valid(\n","          model,\n","          criterion,\n","          valid_loader,\n","          epoch=epoch,\n","          total_epochs=epochs,\n","          batch_size=batch_size\n","      )\n","      valid_loss.append(loss)\n","      valid_acc.append(acc)\n","      print(f'Avg valid Loss: {loss}, Avg valid acc: {acc}')\n","\n","    Plot(\"Loss Curve\", 'Loss', epoch_list, train_loss, valid_loss)\n","    Plot(\"Accuarcy Curve\", 'Acc', epoch_list, train_acc, valid_acc)\n","\n","    preds = predict(test_set, model)\n","    view_pred_result(preds)\n","\n","main()"],"metadata":{"id":"za9NH6rY1x-V","executionInfo":{"status":"aborted","timestamp":1710730152439,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9. Addition: Customize your own model\n","Create your own deep learning model, by define the inner layers in hand-craft<br>\n","Example for VGG16 model: https://arxiv.org/abs/1409.1556"],"metadata":{"id":"ZBzQKwt9F0i-"}},{"cell_type":"code","source":["class VGG16(nn.Module):\n","    def __init__(self, num_classes=12):\n","        super(VGG16, self).__init__()\n","        # input layer\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU()\n","        )\n","\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )\n","\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU()\n","        )\n","\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )\n","\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU())\n","\n","        self.layer6 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","\n","        self.layer7 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )\n","\n","        self.layer8 = nn.Sequential(\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","\n","        self.layer9 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","\n","        self.layer10 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )\n","\n","        self.layer11 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","\n","        self.layer12 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","\n","        self.layer13 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        )\n","\n","        #  classifier\n","        self.fc = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(7*7*512, 4096),\n","            nn.ReLU()\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU()\n","        )\n","\n","        self.fc2= nn.Sequential(\n","            nn.Linear(4096, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.layer5(out)\n","        out = self.layer6(out)\n","        out = self.layer7(out)\n","        out = self.layer8(out)\n","        out = self.layer9(out)\n","        out = self.layer10(out)\n","        out = self.layer11(out)\n","        out = self.layer12(out)\n","        out = self.layer13(out)\n","        out = out.reshape(out.size(0), -1)\n","        out = self.fc(out)\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        return out"],"metadata":{"id":"wKuXQ15OGKLo","executionInfo":{"status":"aborted","timestamp":1710730152439,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test model to debug\n","x = torch.rand(1, 3, 224, 224)\n","model = VGG16(num_classes=12)\n","y = model(x)\n","print(y)"],"metadata":{"id":"kE23DmwHGl9k","executionInfo":{"status":"aborted","timestamp":1710730152439,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Rerun training for VGG16 model"],"metadata":{"id":"ERXYTM7ORcp4"}},{"cell_type":"code","source":["def main():\n","    # initial transform\n","    transform = tsfm.Compose([\n","        tsfm.Resize((224, 224)),\n","        tsfm.ToTensor(),\n","    ])\n","\n","    # initial dataset\n","    whole_set = Train_data(\n","        root_dir=train_dir,\n","        transform=transform\n","    )\n","\n","    test_set = Pred_data(\n","        root_dir=test_dir,\n","        transform=transform\n","    )\n","\n","    # split train valid and initial dataloader\n","    train_set_size = int(len(whole_set) * 0.8)\n","    valid_set_size = len(whole_set) - train_set_size\n","    train_set, valid_set = random_split(whole_set, [train_set_size, valid_set_size])\n","\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    valid_loader = DataLoader(valid_set, batch_size=batch_size)\n","\n","    # initial model\n","    model = VGG16(num_classes=12).cuda()\n","\n","    # initial loss_function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # initial plot values\n","    train_loss, train_acc = [], []\n","    valid_loss, valid_acc = [], []\n","    epoch_list = []\n","\n","    # repeat train and valid epochs times\n","    print(epochs)\n","    for epoch in range(epochs):\n","      epoch_list.append(epoch + 1)\n","\n","      loss, acc = train(\n","          model,\n","          criterion,\n","          optimizer,\n","          train_loader,\n","          epoch=epoch,\n","          total_epochs=epochs,\n","          batch_size=batch_size\n","      )\n","      train_loss.append(loss)\n","      train_acc.append(acc)\n","      print(f'Avg train Loss: {loss}, Avg train acc: {acc}')\n","\n","      loss, acc = valid(\n","          model,\n","          criterion,\n","          valid_loader,\n","          epoch=epoch,\n","          total_epochs=epochs,\n","          batch_size=batch_size\n","      )\n","      valid_loss.append(loss)\n","      valid_acc.append(acc)\n","      print(f'Avg valid Loss: {loss}, Avg valid acc: {acc}')\n","\n","    Plot(\"Loss Curve\", 'Loss', epoch_list, train_loss, valid_loss)\n","    Plot(\"Accuarcy Curve\", 'Acc', epoch_list, train_acc, valid_acc)\n","\n","    preds = predict(test_set, model)\n","    view_pred_result(preds)\n","\n","main()"],"metadata":{"id":"C2brZ16wRjMy","executionInfo":{"status":"aborted","timestamp":1710730152439,"user_tz":-480,"elapsed":6,"user":{"displayName":"陳國恩","userId":"11489219855843201587"}}},"execution_count":null,"outputs":[]}]}